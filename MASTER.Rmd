---
title: "Final Project - MASTER"
last updated: 3/15/18
---

###Data Cleaning and Analysis Tutorial
  
This tutorial will lead you through the steps of cleaning and analyzing survey data in R. This analysis is a part of a larger research project examining the organizing structure and capacity of food pantries. You will find instructions both in the text before each code chunk as well as in comment form within the chunks.
  
#####About the Data  
The data used for this tutorial is from a survey of emergency food assistance providers in the Detroit Metropolitan area from 2012 and 2013. The data was collected at the University of Chicago by a trained interviewer who administered the surveys either by phone or via an online survey tool. Survey questions asked about the organizational characteristics of each agency such as the hours of operation, types of programs offered, and staffing structure. We will also use population data from the American Community Survey from 2010-2014. For the purpose of this tutorial, our analysis will focus on a subset of this data with variables describing the types of programs offered, geographic location, and demographic characteristics.

####Step 1: Create Dataset

The survey data comes in three different Excel files: initial responses from 2/15/13, recoded questions, and addresses. We need to import each of these individually and then merge them together. All of the raw data files are stored on Github and we will temporarily download them for our analysis.

#####Import Survey Data

First we need to import the survey data. We will use the "readxl" package, so make sure this package is installed the first time you use it. We will import the three Excel files individually and place them in separate data frames.

```{r Import Data: Initial, warning=FALSE, message=FALSE}
# Load library
# First time you need to install "readxl": install.packages("readxl")
library(readxl)

# Import initial survey responses from 2_15_13
temp = tempfile(fileext = '.xlsx') #extension for Excel files
dataURL <- "https://github.com/lrsulli/Project/raw/master/Data/fp_survey_2_15_13.xlsx" #link to data
download.file(dataURL, destfile = temp, mode = 'wb') #temporarily download file

data_initial = read_excel(temp, sheet = 1) #get data
```

```{r Import Data: Recodes, warning=FALSE, message=FALSE}
# Load library
library(readxl)

# Recode file for Q12 and Q22 to Q25
temp = tempfile(fileext = '.xlsx') #extension for Excel fie
dataURL <- "https://github.com/lrsulli/Project/raw/master/Data/recode_Q12_Q22toQ25.xlsx" #link to data
download.file(dataURL, destfile = temp, mode = 'wb') #temporarily download file

data_recode = read_excel(temp, sheet = 1) #get data
```

```{r Import Data: Addresses, warning=FALSE, message=FALSE}
# Load library
library(readxl)

# Address list data file
temp = tempfile(fileext = '.xlsx') #extension for Excel fie
dataURL <- "https://github.com/lrsulli/Project/raw/master/Data/address_list_5_20_13.xlsx" #link to data
download.file(dataURL, destfile = temp, mode = 'wb') #temporarily download file

data_address = read_excel(temp, sheet = 1) #get data
```

#####Merge Survey Data

Now that we have each Excel file in a data frame we need to merge them into one data frame for our analysis. To do this we need to have a common variable in each data frame, in this case a unique "STUDYID" was given to each provider when they were surveyed. We will merge using this identifier. We will also prepare each data frame for merging by renaming variables where necessary and only keeping a subset of the variables to keep things simpler.  

First, we will merge the initial survey responses with the recode file:

```{r Merge Data: Recodes, warning=FALSE, message=FALSE}
# Prepare recode file
names(data_recode) #Check names and see what the identifyer variable is called

# We don't need all of these varaibles so we will only keep the necessary ones:
data_recode = data_recode[,c(1,5:6,14:15,19:20)] #only keep necessary variables

# Rename the identifyer variable so it matches the initial data set to keep things simplier:
names(data_recode)[1] <- "STUDYID" #rename to match initial data set

# Merge recode file with initial data set
data = merge(data_initial, data_recode, by="STUDYID")
```

Then we add the address data:

```{r Merge Data: Addresses, warning=FALSE, message=FALSE}
# Prepare address list file
names(data_address)[1] <- "STUDYID" #rename to match inital data set

# Merge address list with initial data set and recodes
data = merge(data, data_address, by="STUDYID")

data = data.frame(data) #convert to data frame
```

#####Import Census Data

We now need to import the Census data so we can analyze the demographics of the populations that the providers serve. This data comes in Stata, or .dta, files which are also stored in Github. We import them the same way as the survey data, just with a different extension. Again, we will temporarily download each file and store them separately in different data frames.  

```{r Import Data: Census 1, warning=FALSE, message=FALSE}
# Load library
# Make sure this package is installed the first time you run the code
library(readstata13)

# Import first of three census data files
temp = tempfile(fileext = '.dta') #extension for Stata data file
dataURL <- "https://github.com/lrsulli/Project/raw/master/Population_Data%204_7_17.dta" #link to data
download.file(dataURL, destfile = temp, mode = 'wb') #temporarily download file

data_census1 = read.dta13(temp) #get data
```

```{r Import Data: Census 2, warning=FALSE, message=FALSE}
# Load library
library(readstata13)

# Import second of three census data files
temp = tempfile(fileext = '.dta') #extension for Stata data file
dataURL <- "https://github.com/lrsulli/Project/raw/master/Census_Data%204_28_17.dta" #link to data
download.file(dataURL, destfile = temp, mode = 'wb') #temporarily download file

data_census2 = read.dta13(temp) #get data
```

```{r Import Data: Census 3, warning=FALSE, message=FALSE}
# Load library
library(readstata13)

# Import third of three census data files
temp = tempfile(fileext = '.dta') #extension for Stata data file
dataURL<-"https://github.com/lrsulli/Project/raw/master/tract10detroit_povertypop_123mile.dta" #link to data
download.file(dataURL, destfile = temp, mode = 'wb') #temporarily download file

data_census3 = read.dta13(temp) #get data
```

#####Merge Census Data

We then need to merge the census data files together as they contain different demographic variables that we need. We will again merge the data frames on a similar variable. For the census data frames this variable will be the census tract number. Then, to add the demographic variables to the survey data we will use the STUDYID variable again, as it is included in one of the census data files.  

First, merge the census data frames:

```{r Merge Data: Census, warning=FALSE, message=FALSE}
# Merge first two census data files (Census1 and Census2)
data_census = merge(data_census1, data_census2, by="tr10fips", all=T) #keep all tracts

# Merge third census data file (Census3 with Census1 and Census2)
data_census = merge(data_census, data_census3, by="tr10fips", all=T) #keep all tracts

data_census = data.frame(data_census) #convert to data frame
```

Then add the census data to the survey data (note that the census data contains all census tracts in Michigan, but we only need the tracts that have food providers in them):

```{r Merge Data: Census with Survey Data, warning=FALSE, message=FALSE}
# Merge Census data and survey data
data = merge(data, data_census, by='STUDYID') #only keep tracts that have organizations

data = data.frame(data) #convert to data frame
```

#####Clean Data Set

Let's check to see what we have:

```{r Check Variable Names, warning=FALSE, message=FALSE, eval=FALSE}
# Check variable names of data
names(data)
```

We don't need all of these variables for our analysis. To make things simpler, we will only keep what we need. We will also make a copy of the original data frame just in case we miss something and want to go back and get it later.

```{r Drop Variables, warning=FALSE, message=FALSE}
# Create a copy of original data set
data_copy = data

# Only keep necessary variables
data = data_copy[,c('tr10fips','STUDYID','Q5','Q6_1','Q6_2','Q6_3','Q6_4','Q6_5','Q7','Q8_1','Q8_2','Q8_3','Q8_4','Q8_5','Q8_6','Q8_7','Q8_8','Q9','Q10_1','Q10_2','Q10_3','Q10_4','Q10_5','Q14','Q15_1','Q15_2','Q15_3','Q15_4','Q15_5','Q15_6','Q15_7','Q15_8','Q15_9','Q15_10','Q15_11','Q15_12','Q15_13','Q12RC','Street.Address','City','State','Zip','County','povpop10071','povpopd71','povpop_1m',"povpopd711mile","povpop_3m","povpopd713mile","nhwht71","pop71","nhwht711mile","nhwht713mile","blk71","blk711mile","blk713mile","hsp71","hsp711mile","hsp713mile","hhsnapfam271","hhsnapfam171","hhsnapfam2711mile","hh711mile","hhsnapfam2713mile","hh713mile")]
```

When we look at the names of the variables, we notice that they are not very helpful in describing the variables themselves. Let's rename the variables to make our analysis easier.

```{r Rename Variables, warning=FALSE, message=FALSE}
# Check variable names of data
names(data)

# Rename identification variables
names(data)[1] <- "tract"
names(data)[2] <- "id"

# Rename types of meal programs
names(data)[3] <- "meals"
names(data)[4] <- "hot_meals"
names(data)[5] <- "home_meals" #Home delivered meals
names(data)[6] <- "community_kitchen"
names(data)[7] <- "child_meals"
names(data)[8] <- "meal_other" #Offer other types of meal programs

# Rename types of grocery programs
names(data)[9] <- "groceries"
names(data)[10] <- "pantry"
names(data)[11] <- "backpack"
names(data)[12] <- "home_groceries" #Home delivered groceries
names(data)[13] <- "mobile_pantry"
names(data)[14] <- "mobile_market"
names(data)[15] <- "supply_other" #Supply food to other programs
names(data)[16] <- "community_garden"
names(data)[17] <- "groceries_other" #Offer other types of grocery programs

# Rename types of food related benefit programs
names(data)[18] <- "food_ben" #Offers food-related benefits
names(data)[19] <- "snap"
names(data)[20] <- "wic"
names(data)[21] <- "school" #School lunch or breakfast
names(data)[22] <- "gift" #Gift card or voucher
names(data)[23] <- "food_ben_other" #Offer other types of food-related benefits

# Rename types of non-food related benefit programs
names(data)[24] <- "nonfood_ben" #Offers non-food related benefits
names(data)[25] <- "job" #Job training or assistance
names(data)[26] <- "house" #Housing assistance
names(data)[27] <- "utility"
names(data)[28] <- "legal"
names(data)[29] <- "educ" #GED or education assistance
names(data)[30] <- "health"
names(data)[31] <- "counseling"
names(data)[32] <- "trans" #Transportation assistance
names(data)[33] <- "cloth" #Clothing or furniture
names(data)[34] <- "ref" #Referrals to other programs
names(data)[35] <- "medicaid" #Medicaid or CHIP
names(data)[36] <- "money" #Financial, tax prep, budgeting
names(data)[37] <- "nonfood_ben_other" #Offer other types of non-food related benefits

# Rename number of clients served per month
names(data)[38] <- "clients"
```

#####Create New Variables

We will need to turn some of our demographic variables into rates for our analysis. To do this, we will create new variables using the "mutate" function. Also, because some of the census tracts are missing within tract poverty demographics, we will instead use the 1-mile estimates for these tracts. We will add these to our main data frame.

```{r Generate Census Variables, warning=FALSE, message=FALSE}
# Load library
# Make sure this package is installed the first time you run the code
library(dplyr)

# Check for missing poverty demographic variables
sum(is.na(data_census$povpop10071)) #there are 962 missing values

# Will use poverty rate within 1 mile for missing within tract poverty rates
data_census$povpop10071[is.na(data_census$povpop10071)] <- 0 #Convert NA's to 0's

data_census <- mutate(data_census, pov_rateT = ifelse(povpop10071==0, povpop100711mile / povpopd711mile, povpop10071 / povpopd71)) #Replace missing values with the correct rate

# Generate poverty rate variables
data <- mutate(data, pov_rateT = povpop10071 / povpopd71) #Rate within census tract
data <- mutate(data, pov_rate1 = povpop_1m / povpopd711mile) #Rate within 1 mile
data <- mutate(data, pov_rate3 = povpop_3m / povpopd713mile) #Rate within 3 miles

# Generate White demographic variables
data <- mutate(data, white_rateT = nhwht71 / pop71) #Rate within census tract
data <- mutate(data, white_rate1 = nhwht711mile / povpopd711mile) #Rate within 1 mile
data <- mutate(data, white_rate3 = nhwht713mile / povpopd713mile) #Rate within 3 miles

# Gnerate Black demographic variables
data <- mutate(data, black_rateT = blk71 / pop71) #Rate within census tract
data <- mutate(data, black_rate1 = blk711mile / povpopd711mile) #Rate within 1 mile
data <- mutate(data, black_rate3 = blk713mile / povpopd713mile) #Rate within 3 miles

# Generate Hispanic demographic variables
data <- mutate(data, hisp_rateT = hsp71 / pop71) #Rate within census tract
data <- mutate(data, hisp_rate1 = hsp711mile / povpopd711mile) #Rate within 1 mile
data <- mutate(data, hisp_rate3 = hsp713mile / povpopd713mile) #Rate within 3 miles

# Generate SNAP rates
data <- mutate(data, snap_rateT = hhsnapfam271 / hhsnapfam171) #Rate within census tract
data <- mutate(data, snap_rate1 = hhsnapfam2711mile / hh711mile) #Rate within 1 mile
data <- mutate(data, snap_rate3 = hhsnapfam2713mile / hh713mile) #Rate within 3 miles
```

It will also be helpful to keep only the counties in the Detroit Metropolitan area, instead of the entire state. However, the census identifying numbers (or FIPS codes) in the data are "tr10fips" which include the state, county, and tract FIPS numbers in one long 10-digit number. We want to create a new variable for just the 3-digit county FIPS code. Later, we will use this new variable for mapping.

```{r Generate County FIPS Variables, warning=FALSE, message=FALSE}
# Need county FIPS variable to focus analysis on Detroit metro area
# We will create this variable in both our main data frame and the complete census data

# Prepare to split census tract identifyer
# Identify the variable types
str(data$tract)
str(data_census$tr10fips)
# They are both character variables so we can continue with the subtr() function

# Create variable with the county and tract fips number by removing the state FIPS number (first 2 digits))
data <- mutate(data, tract_short = substr(tract, 3, 11)) #keep the 3rd through 11th digits
data_census <- mutate(data_census, tract_short = substr(tr10fips, 3, 11)) #keep the 3rd through 11th digits

# Create county FIPS code variable
data <- mutate(data, county_fips = substr(tract, 3, 5)) #keep the 3rd through 5th digits
data_census <- mutate(data_census, county_fips = substr(tr10fips, 3, 5)) #keep the 3rd through 5th digits
```

```{r Save, warning=FALSE, message=FALSE}
# Save a copy of the data in csv format
write.csv(data, "data.csv") #saved to Github repo
```


####Step 2: Explore the Data

Now that we have our data how we want it, let's do some basic analysis. We will look at the dichotomous and continuous variables separately.

#####Explore Dichotomous Variables

We want to determine the prevalence of each type of program or how many food providers offer each major type of program. These variables have simple dichotomous, yes/no, responses. We will use basic functions within R to calculate the statistics and a package called "pander" to format our tables.  

#####Meal Programs

Meal programs include the following services: hot meals, home delivered meals, community kitchens, and meals for children.  

First, the distribution of meal programs:

```{r Meals: Distribution, warning=FALSE, message=FALSE}
# Load library
# Make sure this package is installed the first time you run the code
library(pander)

# Distribution of values
pander(table(data$meals), stlye = 'rmarkdown', caption = "Number of food pantries offering meal programs.")
```

And the frequency of meal programs:

```{r Meals: Frequencies, warning=FALSE, message=FALSE}
# Load library
library(pander)

# Relative Frequencies
pander((prop.table(table(data$meals)))*100, stlye = 'rmarkdown', caption = "Percent of food pantries offering meal programs.", digits = 3)
```

These results show us that 74 food providers, or 30.3% of those surveyed, offer meal programs to their client. On the other hand, a majority or 69.7% representing 170 organizations, do not offer meal programs.

#####Grocery Programs

Grocery programs include the following services: food pantries, backpack programs, home delivered groceries, mobile pantries or markets, supplying food to other programs, or community gardens.  

The distribution of grocery programs is:

```{r Groceries: Distribution, warning=FALSE, message=FALSE}
# Load library
library(pander)

# Distribution of values
pander(table(data$groceries), stlye = 'rmarkdown', caption = "Number of food pantries offering grocery programs.")
```

And the frequency of grocery programs is:

```{r Groceries: Frequencies, warning=FALSE, message=FALSE}
# Load library
library(pander)

# Relative Frequencies
pander((prop.table(table(data$groceries)))*100, stlye = 'rmarkdown', caption = "Percent of food pantries offering grocery programs.", digits = 3)
```

This analysis tells us that grocery programs are more prevalent than meal programs. This trend is seen through 91.0%, or 222 organizations, offering grocery programs compared to just 30.3% offering meal programs.

#####Food Related Benefits

Food related benefits include the following services: Supplemental Nutrition Assistance Program (SNAP), Women, Infant, and Children (WIC), school lunch or breakfast, and gift cards or vouchers.  

First we look at the basic distribution of food related benefit programs:

```{r Food Benefits: Distribution, warning=FALSE, message=FALSE}
# Load library
library(pander)

# Distribution of values
pander(table(data$food_ben), stlye = 'rmarkdown', caption = "Number of food pantries offering food-related benefits.")
```

Then we look at the frequency of food related benefit programs:

```{r Food Benefits: Frequencies, warning=FALSE, message=FALSE}
# Load library
library(pander)

# Relative Frequencies
pander((prop.table(table(data$food_ben)))*100, stlye = 'rmarkdown', caption = "Percent of food pantries offering food-related benefits.", digits = 3)
```

We can see that about half, or 49.2%, of surveyed organizations offer food related benefits to their clients.

#####Non-Food Related Benefits

Non-food related benefits include the following services: job training, housing assistance, utility assistance, legal assistance, GED or other education programs, healthcare, counseling, transportation services, clothing or furniture, referrals to other programs, Medicaid or CHIP, and financial assistance.  

Here is the basic distribution of non-food related benefits:

```{r Non-Food Benefits: Distribution, warning=FALSE, message=FALSE}
# Load library
library(pander)

# Distribution of values
pander(table(data$nonfood_ben), stlye = 'rmarkdown', caption = "Number of food pantries offering non-food related benefits.")
```

And here is the frequency of non-food related benefit programs:

```{r Non-Food Benefits: Frequencies, warning=FALSE, message=FALSE}
# Load library
library(pander)

# Relative Frequencies
pander((prop.table(table(data$nonfood_ben)))*100, stlye = 'rmarkdown', caption = "Percent of food pantries offering non-food related benefits.", digits = 3)
```

This part of the analysis shows us the high prevalence of non-food related benefits within the emergency food assistance network, suggesting the potentially larger role these organizations play in the social safety net. 76.2% of surveyed organizations provide some form of non-food related benefits, while only 23.4% did not provide any of these types of services, and 0.4% (or 1 pantry) did not know or refused to answer the question.

#####Explore Countinuous Variables

We will now look at how to complete a basic analysis of a continuous variable.

#####Average Number of Clients Served per Month

We will look at the average number of clients served per month. This value is self-reported by the organizations on the survey. When a range was given, the mean of the two numbers was used.  

First, we will look at the centrality of the variable:

```{r Clients: Centrality, warning=FALSE, message=FALSE}
# Load library
library(pander)

# Centrality
pander(summary(data$clients), style = 'rmarkdown', caption = "Average number of clients served per month.")
```

We will then use a histogram to visualize the data. Due to the large range and skewness of the variable we will take the log of the variable in order to better visualize the data.

```{r Clients: Histogram, warning=FALSE, message=FALSE}
# Load library
# Make sure this package is installed the first time you run the code
library(ggplot2)

# Histogram
ggplot(data, aes(x=data$clients)) +
  geom_histogram(color = "#1F3552", fill = "#4271AE") + #blue color
  scale_x_log10() + #log of variable
  scale_y_continuous() +
  labs(title="Clients Served") + #title of plot
  labs(x="Log of the Average Number of Clients Served per Month", y="Count") #x-axis label
```

####Step 3: Map the Data

Our main analysis will be examining spatial mismatch theory in light of our survey results. Spatial mismatch theory is often applied to access to employment, but can also be used to explore access to assistance and other resources. In general, the theory states that one cause of poverty is that resources (or employment, assistance, etc.) are not located in areas where low-income individuals live. To explore this theory, we will plot a map of poverty rates and the location of the food providers. In order to best help those in need, ideally these organizations will be located in the communities with the highest poverty rates.  

In order to complete this analysis we will create a map using a shapefile, geocode the food pantry locations, and create a final map overlaying the two.  

#####Getting the Map

First we need to locate a map:

```{r Map Link, warning=FALSE, message=FALSE}
# I found a shapefile of the state of Michigan broken up by census tract
# I saved these files in a compresses folder in Github

# Link to map
compressedMap= "https://github.com/lrsulli/Project/raw/master/map/2010_Census_Tracts_v17a.zip"
```

Then we need to unzip the compressed file:

```{r Unzip Map, warning=FALSE, message=FALSE}
# Unzip compressed map folder
temp=tempfile()
download.file(compressedMap, temp)
unzip(temp)
```

And determine the name of the shapefile we want to plot:

```{r Shapefiles, warning=FALSE, message=FALSE}
# Determine which shapefiles are in folder
(maps = list.files(pattern = 'shp'))
```

We need to select the correct map file using the package "rgdal" so we can map it later:

```{r Select Map, warning=FALSE, message=FALSE}
# Load library
# Make sure this package is installed the first time you run the code
library(rgdal)

# Select the map
tractMap <- readOGR("2010_Census_Tracts_v17a.shp", stringsAsFactors=F)
```

We need to explore the shapefile we selected so we know what we are working with:

```{r Explore Map Data, warning=FALSE, message=FALSE}
names(tractMap) #check variable names
str(tractMap$LINK) #check LINK (FIPS code) variable type

# The census tract number is a character, but we need it in numeric form
# Change LINK (FIPS code) variable into an interger
tractMap$NAME=as.numeric(tractMap$LINK)
```

We do not need to map the entire state so let's just select the counties in the Detroit Metro area. We can do this using the county FIPS codes we created earlier. At this point we will also merge the map files with our census data so we can plot the poverty rates later.

```{r Select Counties, warning=FALSE, message=FALSE}
# Only need counties where pantries are located (Detroit metro area)

# Keep Wayne, Macomb, Oakland, and Washtenaw Counties (FIPS = 163, 099, 125, and 161)
map_counties = subset(tractMap, CNTY_CODE=='163' | CNTY_CODE=='099' | CNTY_CODE=='125' | CNTY_CODE=='161')

# We only need one observation per census tract
data_counties = data_census[!duplicated(data_census[c('tract_short')]),] #drop duplicate tracts

# Merge map data with census data
map_counties = merge(map_counties, data_counties, by.x='LINK', by.y='tract_short', all.x=F)

# Save a copy of the data in csv format
write.csv(map_counties, "map_counties.csv") #saved to Github repo
```

Let's check to see that we have the correct area:

```{r Check County Map, warning=FALSE, message=FALSE}
# Check map to determine if correct counties selected
plot(map_counties, col='black')
```

Now we will geocode the organization locations using the addresses provided. We will do this using code from https://qanda.science/r/3956223_batch-geocoding-with-googleway-r-how-to.jsp and the googleway package. You will need to get a Google API key before completing this step. All directions can be found at the link provided.  

First, we have to make sure that our address variable is formatted correctly. Then, we will geocode the addresses which will return the latitude and longitude coordinates that will need to be formatted into their own variables. Geocoding takes a long time so after completing this step once I recommend writing the output to a csv file, with the STUDYID's, and then working from that csv file. We can then turn the code chunk off so it will not run when we knit the document by adding "eval=FALSE" to the beginning of the chunk.

```{r Geocoding, eval=FALSE, warning=FALSE, message=FALSE}
# Only ran once because it takes awhile, wrote into an .csv file and put in Github repo

# Load library
# Make sure this package is installed the first time you run the code
library(googleway)

# Prepare data_address data frame
data_address$address <- do.call(paste, c(data_address[c("Street Address", "City", "State")], sep = ",")) #correct single line address format

# Got code from https://qanda.science/r/3956223_batch-geocoding-with-googleway-r-how-to.jsp

# API key for Google geocoding
api_key = "AIzaSyC2uRbBeIFBwdSiH6KH69Stx_TUoqPN0bY"

# Geocode addresses (only do once because of time)
res <- apply(data_address, 1, function(x){
  google_geocode(address = x[['address']],
                 key = api_key)
})

# Add geocode results to data_address data frame
data_address$location <- lapply(res, function(x){
  x$results$geometry$location
})

# Create latitude and longitude variables from results
data_address$location = as.character(data_address$location) #destring location variable

data_address <- mutate(data_address, lat = substr(location, 12, 20)) #get latitude numbers
data_address <- mutate(data_address, lon = substr(location, 29, 38)) #get longitude numbers

# Merge latitude and longitude with main data set to get Study ID's
map_address <- merge(data_address, data, by.x='STUDYID', by.y='id')

# Keep only necessary variables
map_address = map_address[,c('STUDYID','lat','lon')]

# Create .csv file with geocode results
write.csv(map_address, "geocoded_addresses.csv") #saved to Github repo
```

We need to import the csv file of latitude and longitude we just created and ensure our variables are the correct type:

```{r Import Data: Geocoded Addresses, warning=FALSE, message=FALSE}
temp = tempfile(fileext = '.csv') #extension for Excel file
dataURL <- "https://raw.githubusercontent.com/lrsulli/Project/master/geocoded_addresses.csv" #link to data
download.file(dataURL, destfile = temp, mode = 'wb') #temporarily download file

map_address = read.csv(temp, header = TRUE, sep = ",") #get data

# Redefine latitude and longitude variables
map_address$lat = as.character(map_address$lat)
map_address$lon = as.character(map_address$lon) 
```

No we can get the map specifications. First, we want to plot the within tract poverty rates:

```{r Select Variable, warning=FALSE, message=FALSE}
# Want to plot within tract poverty rate
varToPLot=map_counties$pov_rateT
```

Then we can set the intervals and color scheme for the color ramp for the poverty rates:

```{r Plot Characteristics, warning=FALSE, message=FALSE}
# Load librarys
# Make sure these packages are installed the first time you run the code
library(RColorBrewer)
library(classInt)

# Number of intervals
numberOfClasses = 5 

# Color pallete
colorForScale='YlGnBu'
colors = brewer.pal(numberOfClasses, colorForScale)

intervals <- classIntervals(varToPLot, numberOfClasses, 
                            style = "quantile",
                            dataPrecision=2)
colorPallette <- findColours(intervals, colors)
```

Now we can map:

```{r Plot, warning=FALSE, message=FALSE}
#Plot characteristics
legendText="Poverty Rate" #legend title
shrinkLegend=0.8 #legend size
title="Poverty Rate by Census Tract"
shrinkPoints=0.35 #point size

plot(map_counties, col = 'grey') #plot background map (tracts without pantries)
plot(map_counties, col = colorPallette, main=title,border='black',add=T) #plot poverty rates

title(main=title)

legend('topright', #location of legend
       fill = attr(colorPallette, "palette"), 
       cex = shrinkLegend, 
       bty = "n",
       title=legendText,
       legend = c("0% - 6%", "6% - 11%", "11% - 18%", "18% - 35%", "35% - 100%")) #legend text

points(map_address$lon, map_address$lat, col = "red", pch =19, cex = shrinkPoints) #add organization locations
```

For the most part, this map shows that the emergency food assistance providers are located in areas with higher poverty rates. However, further more advanced spatial analysis would need to be concluded to confirm this trend.